# Homework2 - Строим свою архитектуру

## Цель Домашнего Задания
В данном домашнем задании от участников курса ожидается небольшой архитектурный документ ( желательно - не более 3-х страниц). Выберите интересный сервис (Twitter / Uber / Linkedin / ваша организация или собственный проект) и разработайте для него архитектуру аналитического хранилища данных. Опишите возможные требования к хранилищу, источники и архитектуру хранения. Приведите пример бизнес-кейса, который можно решить с помощью выбранной вами архитектуры. В решении обязательно должна присутствовать архитектурная схема вашего решения, которая должна объяснять откуда к вам поступают данные, как вы планируете их хранить и как вы планируете их отдавать для решения бизнес-кейса. Описание архитектуры - это стандартное задание на архитектурных собеседованиях для Data Engineer. Выполнив данное задание, вы сможете в будущем воспользоваться этими знаниями для того, чтобы качественно и продуманно создавать дизайн для аналитического хранилища данных или понимать как ваше data-driven приложение ложится в экосистему вашей организации. Для рисования схемы советуем использовать бесплатный сервис draw.io

## Бизнес кейс
На основе исторических данных физических лиц необходимо прогнозировать отток клиентов при помощи модели машинного обучения. Спрогнозированные данные выгружать в CSV или внешнюю систему для дальнейшей работы с ФЛ через КЦ или другие каналы связи с клиентами.

## Архитектура решения

Была разработана [архитектура решения](https://github.com/adm-8/otus-de-andreevds-2019-11/blob/master/HW2_Lesson2/DE-2019-11-AndreevDS-HW2.pdf) , которая подразуевает, что для модели были заранее определены необходимые признаки и наша задача только разработать процесс сбора, трансформации данных, обучения модели на полученных данных и запуска процесса прогнозирования оттока и выгрузка этих результатов в CSV\Внешнюю БД.

Считается, что ФЛ ушёл из банка в том случае, если он более N недель не проводил транзакий. Поэтому основным источником данных для нас будет БД с транакциями (ИД Транзакции). Помимо самих транзакций для прогнозирования нам могут быть полезны такие источники данных как: обращения ФЛ в службу поддержки (ИД Обращения в саппорт), история входящих смс (ИД СМС) и история походов на сайты (ИД Хосты). Оркестрировать процессы будем при помощи Apache Airflow.

### Шаг 1. формируем Bronze Layer.
В первую очередь нам будет неоходимо переливть данные из источников во внутренее хранилище. Данные будем лить батчами по ночам, где дата\время создания записей больше чем дата\время последнего успешно отработанного батча. Данные из источников данных (ИД) в хранилище данных (ХД) будем лить один к одному, без изменений. 

### Шаг 2. формируем Silver Layer.
После того как мы импортировали данные к себе в хранилище, мы можем преобразовать их. Основная цель трансформации данных из каждого источник - "перевернуть" данные из вида 1:M в "плоский вид", где одна строка будет содержать ключ для ФЛ (например его идентификатор в системе Way4) и набор из N признаков. Где каждый из N признаков - бинарное значение, сообщающее нам о том, было ли действие N недель назад. Таким образом, необходимо будет следующие преобразования (подразумевается, что какие именно признаки генерим - это за нас уже придумали DS или аналитики):

* Была ли транзакия 
* Было ли обращение в саппорт.
* Приходили ли СМС с искаомым текстом (предложения от конкурентов)
* Были ли переходы с мобильного на сайты конкурентов
* На основе транзакций необходимо сформировать признак активности клиента для обучения

### Шаг 3. формируем Golden Layer.
После того как мы получили 4 наборов данных с ключевой колонкой + N признаков, можем объединтиь их в один набор по ключевому значению. Плюс у нас должен быть набор данных с целевым значением для прогнозирования.

### Шаг 4. Обучение модели. 
На основе данных из п.3 обучаем нашу модель. Для текущего домашнего задания будем считать, что конкретная модель уже выбрана, гиперпараметры подобраны.

### Шаг 5. Получение предсказаний и их выгрузка
После того как мы обучили нашу модель, мы сможем запустить процесс прогноза оттока ФЛ в ближайшее время. Из результатов прогноза получаем ключи ФЛ, которые "оттекут" и выгружаем их либо CSV-файлом для внешней системы, либо заливаем эти данные сразу во внешнюю БД, тут уже стоит делать так как будет удобнее коллегам, кто будет обрабатывать уходящих ФЛ.
